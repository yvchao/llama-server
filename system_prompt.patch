diff --git a/examples/server/server.cpp b/examples/server/server.cpp
index fd75532..0eb6917 100644
--- a/examples/server/server.cpp
+++ b/examples/server/server.cpp
@@ -45,28 +45,27 @@ static bool server_verbose = false;
 #if SERVER_VERBOSE != 1
 #define LOG_VERBOSE(MSG, ...)
 #else
-#define LOG_VERBOSE(MSG, ...)                                            \
-    do                                                                   \
-    {                                                                    \
-        if (server_verbose)                                              \
-        {                                                                \
-            server_log("VERBOSE", __func__, __LINE__, MSG, __VA_ARGS__); \
-        }                                                                \
+#define LOG_VERBOSE(MSG, ...)                                                                                          \
+    do                                                                                                                 \
+    {                                                                                                                  \
+        if (server_verbose)                                                                                            \
+        {                                                                                                              \
+            server_log("VERBOSE", __func__, __LINE__, MSG, __VA_ARGS__);                                               \
+        }                                                                                                              \
     } while (0)
 #endif
 
-#define LOG_ERROR(  MSG, ...) server_log("ERROR",   __func__, __LINE__, MSG, __VA_ARGS__)
+#define LOG_ERROR(MSG, ...) server_log("ERROR", __func__, __LINE__, MSG, __VA_ARGS__)
 #define LOG_WARNING(MSG, ...) server_log("WARNING", __func__, __LINE__, MSG, __VA_ARGS__)
-#define LOG_INFO(   MSG, ...) server_log("INFO",    __func__, __LINE__, MSG, __VA_ARGS__)
+#define LOG_INFO(MSG, ...) server_log("INFO", __func__, __LINE__, MSG, __VA_ARGS__)
 
 //
 // base64 utils (TODO: move to common in the future)
 //
 
-static const std::string base64_chars =
-             "ABCDEFGHIJKLMNOPQRSTUVWXYZ"
-             "abcdefghijklmnopqrstuvwxyz"
-             "0123456789+/";
+static const std::string base64_chars = "ABCDEFGHIJKLMNOPQRSTUVWXYZ"
+                                        "abcdefghijklmnopqrstuvwxyz"
+                                        "0123456789+/";
 
 static inline bool is_base64(uint8_t c)
 {
@@ -88,17 +87,18 @@ static std::vector<uint8_t> base64_decode(std::string const &encoded_string)
 
     while (in_len-- && (encoded_string[in_] != '=') && is_base64(encoded_string[in_]))
     {
-        char_array_4[i++] = encoded_string[in_]; in_++;
+        char_array_4[i++] = encoded_string[in_];
+        in_++;
         if (i == 4)
         {
-            for (i = 0; i <4; i++)
+            for (i = 0; i < 4; i++)
             {
                 char_array_4[i] = base64_chars.find(char_array_4[i]);
             }
 
-            char_array_3[0] = ((char_array_4[0]      ) << 2) + ((char_array_4[1] & 0x30) >> 4);
+            char_array_3[0] = ((char_array_4[0]) << 2) + ((char_array_4[1] & 0x30) >> 4);
             char_array_3[1] = ((char_array_4[1] & 0xf) << 4) + ((char_array_4[2] & 0x3c) >> 2);
-            char_array_3[2] = ((char_array_4[2] & 0x3) << 6) +   char_array_4[3];
+            char_array_3[2] = ((char_array_4[2] & 0x3) << 6) + char_array_4[3];
 
             for (i = 0; (i < 3); i++)
             {
@@ -110,19 +110,19 @@ static std::vector<uint8_t> base64_decode(std::string const &encoded_string)
 
     if (i)
     {
-        for (j = i; j <4; j++)
+        for (j = i; j < 4; j++)
         {
             char_array_4[j] = 0;
         }
 
-        for (j = 0; j <4; j++)
+        for (j = 0; j < 4; j++)
         {
             char_array_4[j] = base64_chars.find(char_array_4[j]);
         }
 
-        char_array_3[0] = ((char_array_4[0]      ) << 2) + ((char_array_4[1] & 0x30) >> 4);
+        char_array_3[0] = ((char_array_4[0]) << 2) + ((char_array_4[1] & 0x30) >> 4);
         char_array_3[1] = ((char_array_4[1] & 0xf) << 4) + ((char_array_4[2] & 0x3c) >> 2);
-        char_array_3[2] = ((char_array_4[2] & 0x3) << 6) +   char_array_4[3];
+        char_array_3[2] = ((char_array_4[2] & 0x3) << 6) + char_array_4[3];
 
         for (j = 0; (j < i - 1); j++)
         {
@@ -137,12 +137,14 @@ static std::vector<uint8_t> base64_decode(std::string const &encoded_string)
 // parallel
 //
 
-enum task_type {
+enum task_type
+{
     COMPLETION_TASK,
     CANCEL_TASK
 };
 
-struct task_server {
+struct task_server
+{
     int id;
     int target_id;
     task_type type;
@@ -151,7 +153,8 @@ struct task_server {
     bool embedding_mode = false;
 };
 
-struct task_result {
+struct task_result
+{
     int id;
     bool stop;
     bool error;
@@ -174,12 +177,12 @@ enum slot_command
 
 struct slot_params
 {
-    bool stream       = true;
+    bool stream = true;
     bool cache_prompt = false; // remember the prompt to avoid reprocessing all prompt
 
-    uint32_t seed      = -1; // RNG seed
-    int32_t  n_keep    =  0; // number of tokens to keep from initial prompt
-    int32_t  n_predict = -1; // new tokens to predict
+    uint32_t seed = -1;     // RNG seed
+    int32_t n_keep = 0;     // number of tokens to keep from initial prompt
+    int32_t n_predict = -1; // new tokens to predict
 
     std::vector<std::string> antiprompt;
 
@@ -192,7 +195,7 @@ struct slot_image
     int32_t id;
 
     bool request_encode_image = false;
-    float* image_embedding = nullptr;
+    float *image_embedding = nullptr;
     int32_t image_tokens = 0;
 
     clip_image_u8 img_data;
@@ -231,12 +234,10 @@ enum stop_type
 
 static bool ends_with(const std::string &str, const std::string &suffix)
 {
-    return str.size() >= suffix.size() &&
-           0 == str.compare(str.size() - suffix.size(), suffix.size(), suffix);
+    return str.size() >= suffix.size() && 0 == str.compare(str.size() - suffix.size(), suffix.size(), suffix);
 }
 
-static size_t find_partial_stop_string(const std::string &stop,
-                                       const std::string &text)
+static size_t find_partial_stop_string(const std::string &stop, const std::string &text)
 {
     if (!text.empty() && !stop.empty())
     {
@@ -257,8 +258,7 @@ static size_t find_partial_stop_string(const std::string &stop,
 }
 
 // TODO: reuse llama_detokenize
-template <class Iter>
-static std::string tokens_to_str(llama_context *ctx, Iter begin, Iter end)
+template <class Iter> static std::string tokens_to_str(llama_context *ctx, Iter begin, Iter end)
 {
     std::string ret;
     for (; begin != end; ++begin)
@@ -268,16 +268,11 @@ static std::string tokens_to_str(llama_context *ctx, Iter begin, Iter end)
     return ret;
 }
 
-static void server_log(const char *level, const char *function, int line,
-                       const char *message, const nlohmann::ordered_json &extra)
+static void server_log(const char *level, const char *function, int line, const char *message,
+                       const nlohmann::ordered_json &extra)
 {
-    nlohmann::ordered_json log
-    {
-        {"timestamp", time(nullptr)},
-        {"level",     level},
-        {"function",  function},
-        {"line",      line},
-        {"message",   message},
+    nlohmann::ordered_json log{
+        {"timestamp", time(nullptr)}, {"level", level}, {"function", function}, {"line", line}, {"message", message},
     };
 
     if (!extra.empty())
@@ -316,28 +311,24 @@ static json probs_vector_to_json(const llama_context *ctx, const std::vector<com
         for (const auto &p : prob.probs)
         {
             std::string tok_str = tokens_to_output_formatted_string(ctx, p.tok);
-            probs_for_token.push_back(json
-            {
+            probs_for_token.push_back(json{
                 {"tok_str", tok_str},
-                {"prob",    p.prob},
+                {"prob", p.prob},
             });
         }
         std::string tok_str = tokens_to_output_formatted_string(ctx, prob.tok);
         out.push_back(json{
             {"content", tok_str},
-            {"probs",   probs_for_token},
+            {"probs", probs_for_token},
         });
     }
     return out;
 }
 
-template <typename T>
-static T json_value(const json &body, const std::string &key, const T &default_value)
+template <typename T> static T json_value(const json &body, const std::string &key, const T &default_value)
 {
     // Fallback null to default value
-    return body.contains(key) && !body.at(key).is_null()
-        ? body.value(key, default_value)
-        : default_value;
+    return body.contains(key) && !body.at(key).is_null() ? body.value(key, default_value) : default_value;
 }
 
 struct llama_client_slot
@@ -354,15 +345,15 @@ struct llama_client_slot
     int64_t t_last_used = -1;
 
     // generation props
-    int32_t n_ctx       = 0;  // context size per slot
-    int32_t n_past      = 0;
-    int32_t n_decoded   = 0;
+    int32_t n_ctx = 0; // context size per slot
+    int32_t n_past = 0;
+    int32_t n_decoded = 0;
     int32_t n_remaining = -1;
-    int32_t i_batch     = -1;
+    int32_t i_batch = -1;
 
-    int32_t num_prompt_tokens           = 0;
+    int32_t num_prompt_tokens = 0;
     int32_t num_prompt_tokens_processed = 0;
-    int32_t multibyte_pending           = 0;
+    int32_t multibyte_pending = 0;
 
     json prompt;
     std::string generated_text;
@@ -395,21 +386,22 @@ struct llama_client_slot
     int64_t t_start_genereration;
 
     double t_prompt_processing; // ms
-    double t_token_generation; // ms
-
-    void reset() {
-        num_prompt_tokens      = 0;
-        generated_text         = "";
-        truncated              = false;
-        stopped_eos            = false;
-        stopped_word           = false;
-        stopped_limit          = false;
-        stopping_word          = "";
-        multibyte_pending      = 0;
-        n_past                 = 0;
-        sent_count             = 0;
+    double t_token_generation;  // ms
+
+    void reset()
+    {
+        num_prompt_tokens = 0;
+        generated_text = "";
+        truncated = false;
+        stopped_eos = false;
+        stopped_word = false;
+        stopped_limit = false;
+        stopping_word = "";
+        multibyte_pending = 0;
+        n_past = 0;
+        sent_count = 0;
         sent_token_probs_index = 0;
-        infill                 = false;
+        infill = false;
 
         generated_token_probs.clear();
 
@@ -424,9 +416,10 @@ struct llama_client_slot
         // llama_set_rng_seed(ctx, params.seed); in batched the seed matter???????
     }
 
-    bool has_budget(gpt_params &global_params) {
+    bool has_budget(gpt_params &global_params)
+    {
         n_remaining = -1;
-        if(params.n_predict != -1)
+        if (params.n_predict != -1)
         {
             n_remaining = params.n_predict - n_decoded;
         }
@@ -437,15 +430,18 @@ struct llama_client_slot
         return n_remaining > 0 || n_remaining == -1; // no budget || limitless
     }
 
-    bool available() const {
+    bool available() const
+    {
         return state == IDLE && command == NONE;
     }
 
-    bool is_processing() const {
+    bool is_processing() const
+    {
         return (state == IDLE && command == LOAD_PROMPT) || state == PROCESSING;
     }
 
-    void add_token_string(const completion_token_output &token) {
+    void add_token_string(const completion_token_output &token)
+    {
         if (command == RELEASE)
         {
             return;
@@ -454,7 +450,8 @@ struct llama_client_slot
         generated_token_probs.push_back(token);
     }
 
-    void release() {
+    void release()
+    {
         if (state == IDLE || state == PROCESSING)
         {
             t_token_generation = (ggml_time_us() - t_start_genereration) / 1e3;
@@ -462,27 +459,31 @@ struct llama_client_slot
         }
     }
 
-    json get_formated_timings() {
-        return json
-        {
-            {"prompt_n",               num_prompt_tokens_processed},
-            {"prompt_ms",              t_prompt_processing},
-            {"prompt_per_token_ms",    t_prompt_processing / num_prompt_tokens_processed},
-            {"prompt_per_second",      1e3 / t_prompt_processing * num_prompt_tokens_processed},
-
-            {"predicted_n",            n_decoded},
-            {"predicted_ms",           t_token_generation},
+    json get_formated_timings()
+    {
+        return json{
+            {"prompt_n", num_prompt_tokens_processed},
+            {"prompt_ms", t_prompt_processing},
+            {"prompt_per_token_ms", t_prompt_processing / num_prompt_tokens_processed},
+            {"prompt_per_second", 1e3 / t_prompt_processing * num_prompt_tokens_processed},
+
+            {"predicted_n", n_decoded},
+            {"predicted_ms", t_token_generation},
             {"predicted_per_token_ms", t_token_generation / n_decoded},
-            {"predicted_per_second",   1e3 / t_token_generation * n_decoded},
+            {"predicted_per_second", 1e3 / t_token_generation * n_decoded},
         };
     }
 
-    void print_timings() {
+    void print_timings()
+    {
         LOG_TEE("\n");
         LOG_TEE("%s: prompt eval time = %10.2f ms / %5d tokens (%8.2f ms per token, %8.2f tokens per second)\n",
-            __func__, t_prompt_processing, num_prompt_tokens_processed, t_prompt_processing / num_prompt_tokens_processed, 1e3 / t_prompt_processing * num_prompt_tokens_processed);
+                __func__, t_prompt_processing, num_prompt_tokens_processed,
+                t_prompt_processing / num_prompt_tokens_processed,
+                1e3 / t_prompt_processing * num_prompt_tokens_processed);
         LOG_TEE("%s:        eval time = %10.2f ms / %5d runs   (%8.2f ms per token, %8.2f tokens per second)\n",
-            __func__, t_token_generation, n_decoded,t_token_generation / n_decoded, 1e3 / t_token_generation * n_decoded);
+                __func__, t_token_generation, n_decoded, t_token_generation / n_decoded,
+                1e3 / t_token_generation * n_decoded);
         LOG_TEE("%s:       total time = %10.2f ms\n", __func__, t_prompt_processing + t_token_generation);
     }
 };
@@ -498,20 +499,20 @@ struct llama_server_context
 
     llama_batch batch;
 
-    bool multimodal         = false;
-    bool clean_kv_cache     = true;
+    bool multimodal = false;
+    bool clean_kv_cache = true;
     bool all_slots_are_idle = false;
 
     int32_t id_gen;
-    int32_t n_ctx;  // total context for all clients / slots
+    int32_t n_ctx; // total context for all clients / slots
 
     // system prompt
     bool system_need_update = false;
 
-    std::string              system_prompt;
+    std::string system_prompt;
     std::vector<llama_token> system_tokens;
 
-    std::string name_user;      // this should be the antiprompt
+    std::string name_user; // this should be the antiprompt
     std::string name_assistant;
 
     // slots / clients
@@ -539,16 +540,19 @@ struct llama_server_context
     bool load_model(const gpt_params &params_)
     {
         params = params_;
-        if (!params.mmproj.empty()) {
+        if (!params.mmproj.empty())
+        {
             multimodal = true;
             LOG_TEE("Multi Modal Mode Enabled");
-            clp_ctx = clip_model_load(params.mmproj.c_str(), /*verbosity=*/ 1);
-            if(clp_ctx == nullptr) {
+            clp_ctx = clip_model_load(params.mmproj.c_str(), /*verbosity=*/1);
+            if (clp_ctx == nullptr)
+            {
                 LOG_ERROR("unable to load clip model", {{"model", params.mmproj}});
                 return false;
             }
 
-            if (params.n_ctx < 2048) { // request larger context for the image embedding
+            if (params.n_ctx < 2048)
+            { // request larger context for the image embedding
                 params.n_ctx = 2048;
             }
         }
@@ -560,11 +564,15 @@ struct llama_server_context
             return false;
         }
 
-        if (multimodal) {
+        if (multimodal)
+        {
             const int n_embd_clip = clip_n_mmproj_embd(clp_ctx);
-            const int n_embd_llm  = llama_n_embd(model);
-            if (n_embd_clip != n_embd_llm) {
-                LOG_TEE("%s: embedding dim of the multimodal projector (%d) is not equal to that of LLaMA (%d). Make sure that you use the correct mmproj file.\n", __func__, n_embd_clip, n_embd_llm);
+            const int n_embd_llm = llama_n_embd(model);
+            if (n_embd_clip != n_embd_llm)
+            {
+                LOG_TEE("%s: embedding dim of the multimodal projector (%d) is not equal to that of LLaMA (%d). Make "
+                        "sure that you use the correct mmproj file.\n",
+                        __func__, n_embd_clip, n_embd_llm);
                 llama_free(ctx);
                 llama_free_model(model);
                 return false;
@@ -576,7 +584,8 @@ struct llama_server_context
         return true;
     }
 
-    void initialize() {
+    void initialize()
+    {
         id_gen = 0;
 
         // create slots
@@ -599,12 +608,17 @@ struct llama_server_context
 
         batch = llama_batch_init(n_ctx, 0, params.n_parallel);
 
-        // empty system prompt
-        system_prompt = "";
-        system_tokens.clear();
+        // The system_prompt may come from the command line arguments.
+        // Check if it is empty before clear it.
+        if (system_prompt.empty())
+        {
+            // empty system prompt
+            system_prompt = "";
+            system_tokens.clear();
+        }
     }
 
-    std::vector<llama_token> tokenize(const json & json_prompt, bool add_bos) const
+    std::vector<llama_token> tokenize(const json &json_prompt, bool add_bos) const
     {
         // If `add_bos` is true, we only add BOS, when json_prompt is a string,
         // or the first element of the json_prompt array is a string.
@@ -613,7 +627,7 @@ struct llama_server_context
         if (json_prompt.is_array())
         {
             bool first = true;
-            for (const auto& p : json_prompt)
+            for (const auto &p : json_prompt)
             {
                 if (p.is_string())
                 {
@@ -649,11 +663,12 @@ struct llama_server_context
         return prompt_tokens;
     }
 
-    llama_client_slot* get_slot(int id) {
+    llama_client_slot *get_slot(int id)
+    {
         int64_t t_last = ggml_time_us();
         llama_client_slot *last_used = nullptr;
 
-        for (llama_client_slot & slot : slots)
+        for (llama_client_slot &slot : slots)
         {
             if (slot.id == id && slot.available())
             {
@@ -670,30 +685,31 @@ struct llama_server_context
         return last_used;
     }
 
-    bool launch_slot_with_data(llama_client_slot* &slot, json data) {
+    bool launch_slot_with_data(llama_client_slot *&slot, json data)
+    {
         slot_params default_params;
         llama_sampling_params default_sparams;
 
-        slot->params.stream           = json_value(data, "stream",            false);
-        slot->params.cache_prompt     = json_value(data, "cache_prompt",      false);
-        slot->params.n_predict        = json_value(data, "n_predict",         default_params.n_predict);
-        slot->sparams.top_k           = json_value(data, "top_k",             default_sparams.top_k);
-        slot->sparams.top_p           = json_value(data, "top_p",             default_sparams.top_p);
-        slot->sparams.tfs_z           = json_value(data, "tfs_z",             default_sparams.tfs_z);
-        slot->sparams.typical_p       = json_value(data, "typical_p",         default_sparams.typical_p);
-        slot->sparams.temp            = json_value(data, "temperature",       default_sparams.temp);
-        slot->sparams.penalty_last_n  = json_value(data, "repeat_last_n",     default_sparams.penalty_last_n);
-        slot->sparams.penalty_repeat  = json_value(data, "repeat_penalty",    default_sparams.penalty_repeat);
-        slot->sparams.penalty_freq    = json_value(data, "frequency_penalty", default_sparams.penalty_freq);
-        slot->sparams.penalty_present = json_value(data, "presence_penalty",  default_sparams.penalty_present);
-        slot->sparams.mirostat        = json_value(data, "mirostat",          default_sparams.mirostat);
-        slot->sparams.mirostat_tau    = json_value(data, "mirostat_tau",      default_sparams.mirostat_tau);
-        slot->sparams.mirostat_eta    = json_value(data, "mirostat_eta",      default_sparams.mirostat_eta);
-        slot->sparams.penalize_nl     = json_value(data, "penalize_nl",       default_sparams.penalize_nl);
-        slot->params.n_keep           = json_value(data, "n_keep",            slot->params.n_keep);
-        slot->params.seed             = json_value(data, "seed",              default_params.seed);
-        slot->sparams.grammar         = json_value(data, "grammar",           default_sparams.grammar);
-        slot->sparams.n_probs         = json_value(data, "n_probs",           default_sparams.n_probs);
+        slot->params.stream = json_value(data, "stream", false);
+        slot->params.cache_prompt = json_value(data, "cache_prompt", false);
+        slot->params.n_predict = json_value(data, "n_predict", default_params.n_predict);
+        slot->sparams.top_k = json_value(data, "top_k", default_sparams.top_k);
+        slot->sparams.top_p = json_value(data, "top_p", default_sparams.top_p);
+        slot->sparams.tfs_z = json_value(data, "tfs_z", default_sparams.tfs_z);
+        slot->sparams.typical_p = json_value(data, "typical_p", default_sparams.typical_p);
+        slot->sparams.temp = json_value(data, "temperature", default_sparams.temp);
+        slot->sparams.penalty_last_n = json_value(data, "repeat_last_n", default_sparams.penalty_last_n);
+        slot->sparams.penalty_repeat = json_value(data, "repeat_penalty", default_sparams.penalty_repeat);
+        slot->sparams.penalty_freq = json_value(data, "frequency_penalty", default_sparams.penalty_freq);
+        slot->sparams.penalty_present = json_value(data, "presence_penalty", default_sparams.penalty_present);
+        slot->sparams.mirostat = json_value(data, "mirostat", default_sparams.mirostat);
+        slot->sparams.mirostat_tau = json_value(data, "mirostat_tau", default_sparams.mirostat_tau);
+        slot->sparams.mirostat_eta = json_value(data, "mirostat_eta", default_sparams.mirostat_eta);
+        slot->sparams.penalize_nl = json_value(data, "penalize_nl", default_sparams.penalize_nl);
+        slot->params.n_keep = json_value(data, "n_keep", slot->params.n_keep);
+        slot->params.seed = json_value(data, "seed", default_params.seed);
+        slot->sparams.grammar = json_value(data, "grammar", default_sparams.grammar);
+        slot->sparams.n_probs = json_value(data, "n_probs", default_sparams.n_probs);
 
         // infill
         if (data.count("input_prefix") != 0)
@@ -781,12 +797,15 @@ struct llama_server_context
                     int width, height, channels;
                     std::vector<uint8_t> image_buffer = base64_decode(data_b64);
                     data_b64.clear();
-                    auto data = stbi_load_from_memory(image_buffer.data(), image_buffer.size(), &width, &height, &channels, 3);
-                    if (!data) {
+                    auto data =
+                        stbi_load_from_memory(image_buffer.data(), image_buffer.size(), &width, &height, &channels, 3);
+                    if (!data)
+                    {
                         LOG_TEE("slot %i - failed to load image [id: %i]\n", slot->id, img_sl.id);
                         return false;
                     }
-                    LOG_TEE("slot %i - image loaded [id: %i] resolution (%i x %i)\n", slot->id, img_sl.id, width, height);
+                    LOG_TEE("slot %i - image loaded [id: %i] resolution (%i x %i)\n", slot->id, img_sl.id, width,
+                            height);
                     img_sl.img_data.nx = width;
                     img_sl.img_data.ny = height;
                     img_sl.img_data.size = width * height * 3;
@@ -797,13 +816,15 @@ struct llama_server_context
                     slot->images.push_back(img_sl);
                 }
                 // process prompt
-                // example: system prompt [img-102] user [img-103] describe [img-134] -> [{id: 102, prefix: 'system prompt '}, {id: 103, prefix: ' user '}, {id: 134, prefix: ' describe '}]}
+                // example: system prompt [img-102] user [img-103] describe [img-134] -> [{id: 102, prefix: 'system
+                // prompt '}, {id: 103, prefix: ' user '}, {id: 134, prefix: ' describe '}]}
                 if (slot->images.size() > 0 && !slot->prompt.is_array())
                 {
                     std::string prompt = slot->prompt.get<std::string>();
                     size_t pos = 0, begin_prefix = 0;
                     std::string pattern = "[img-";
-                    while ((pos = prompt.find(pattern, pos)) != std::string::npos) {
+                    while ((pos = prompt.find(pattern, pos)) != std::string::npos)
+                    {
                         size_t end_prefix = pos;
                         pos += pattern.length();
                         size_t end_pos = prompt.find("]", pos);
@@ -816,19 +837,23 @@ struct llama_server_context
                                 bool found = false;
                                 for (slot_image &img : slot->images)
                                 {
-                                    if (img.id == img_id) {
+                                    if (img.id == img_id)
+                                    {
                                         found = true;
                                         img.prefix_prompt = prompt.substr(begin_prefix, end_prefix - begin_prefix);
                                         begin_prefix = end_pos + 1;
                                         break;
                                     }
                                 }
-                                if (!found) {
+                                if (!found)
+                                {
                                     LOG_TEE("ERROR: Image with id: %i, not found.\n", img_id);
                                     slot->images.clear();
                                     return false;
                                 }
-                            } catch (const std::invalid_argument& e) {
+                            }
+                            catch (const std::invalid_argument &e)
+                            {
                                 LOG_TEE("Invalid image number id in prompt\n");
                                 slot->images.clear();
                                 return false;
@@ -856,22 +881,24 @@ struct llama_server_context
         return true;
     }
 
-    void kv_cache_clear() {
+    void kv_cache_clear()
+    {
         // clear the entire KV cache
         llama_kv_cache_clear(ctx);
         clean_kv_cache = false;
     }
 
-    void update_system_prompt() {
+    void update_system_prompt()
+    {
         system_tokens = ::llama_tokenize(ctx, system_prompt, true);
 
         llama_batch_clear(batch);
 
         kv_cache_clear();
 
-        for (int i = 0; i < (int) system_tokens.size(); ++i)
+        for (int i = 0; i < (int)system_tokens.size(); ++i)
         {
-            llama_batch_add(batch, system_tokens[i], i, { 0 }, false);
+            llama_batch_add(batch, system_tokens[i], i, {0}, false);
         }
 
         if (llama_decode(ctx, batch) != 0)
@@ -890,7 +917,8 @@ struct llama_server_context
         system_need_update = false;
     }
 
-    void notify_system_prompt_changed() {
+    void notify_system_prompt_changed()
+    {
         // release all slots
         for (llama_client_slot &slot : slots)
         {
@@ -900,9 +928,10 @@ struct llama_server_context
         system_need_update = true;
     }
 
-    void process_system_prompt_data(const json &sys_props) {
-        system_prompt  = sys_props.value("prompt", "");
-        name_user      = sys_props.value("anti_prompt", "");
+    void process_system_prompt_data(const json &sys_props)
+    {
+        system_prompt = sys_props.value("prompt", "");
+        name_user = sys_props.value("anti_prompt", "");
         name_assistant = sys_props.value("assistant_name", "");
 
         if (slots.size() > 0)
@@ -911,8 +940,8 @@ struct llama_server_context
         }
     }
 
-    static size_t find_stopping_strings(const std::string &text, const size_t last_token_size,
-                                        const stop_type type, llama_client_slot &slot)
+    static size_t find_stopping_strings(const std::string &text, const size_t last_token_size, const stop_type type,
+                                        llama_client_slot &slot)
     {
         size_t stop_pos = std::string::npos;
 
@@ -929,8 +958,7 @@ struct llama_server_context
             {
                 pos = find_partial_stop_string(word, text);
             }
-            if (pos != std::string::npos &&
-                (stop_pos == std::string::npos || pos < stop_pos))
+            if (pos != std::string::npos && (stop_pos == std::string::npos || pos < stop_pos))
             {
                 if (type == STOP_FULL)
                 {
@@ -945,7 +973,8 @@ struct llama_server_context
         return stop_pos;
     }
 
-    bool process_token(completion_token_output &result, llama_client_slot &slot) {
+    bool process_token(completion_token_output &result, llama_client_slot &slot)
+    {
         // remember which tokens were sampled - used for repetition penalties during sampling
         const std::string token_str = llama_token_to_piece(ctx, result.tok);
         slot.sampled = result.tok;
@@ -991,9 +1020,7 @@ struct llama_server_context
             if (stop_pos != std::string::npos)
             {
                 is_stop_full = true;
-                slot.generated_text.erase(
-                    slot.generated_text.begin() + pos + stop_pos,
-                    slot.generated_text.end());
+                slot.generated_text.erase(slot.generated_text.begin() + pos + stop_pos, slot.generated_text.end());
                 pos = std::min(slot.sent_count, slot.generated_text.size());
             }
             else
@@ -1060,7 +1087,7 @@ struct llama_server_context
                 continue;
             }
             clip_image_f32 img_res;
-            if (!clip_image_preprocess(clp_ctx, &img.img_data, &img_res, /*pad2square =*/ true))
+            if (!clip_image_preprocess(clp_ctx, &img.img_data, &img_res, /*pad2square =*/true))
             {
                 LOG_TEE("Error processing the given image");
                 clip_free(clp_ctx);
@@ -1092,7 +1119,7 @@ struct llama_server_context
         task_result res;
         res.id = id;
         res.error = true;
-        res.result_json = { { "content", error } };
+        res.result_json = {{"content", error}};
         queue_results.push_back(res);
     }
 
@@ -1104,33 +1131,33 @@ struct llama_server_context
     json get_formated_generation(llama_client_slot &slot)
     {
         const auto eos_bias = slot.sparams.logit_bias.find(llama_token_eos(model));
-        const bool ignore_eos = eos_bias != slot.sparams.logit_bias.end() &&
-                                eos_bias->second < 0.0f && std::isinf(eos_bias->second);
-        return json {
-            {"n_ctx",             slot.n_ctx},
-            {"model",             params.model_alias},
-            {"seed",              slot.params.seed},
-            {"temp",              slot.sparams.temp},
-            {"top_k",             slot.sparams.top_k},
-            {"top_p",             slot.sparams.top_p},
-            {"tfs_z",             slot.sparams.tfs_z},
-            {"typical_p",         slot.sparams.typical_p},
-            {"repeat_last_n",     slot.sparams.penalty_last_n},
-            {"repeat_penalty",    slot.sparams.penalty_repeat},
-            {"presence_penalty",  slot.sparams.penalty_present},
+        const bool ignore_eos =
+            eos_bias != slot.sparams.logit_bias.end() && eos_bias->second < 0.0f && std::isinf(eos_bias->second);
+        return json{
+            {"n_ctx", slot.n_ctx},
+            {"model", params.model_alias},
+            {"seed", slot.params.seed},
+            {"temp", slot.sparams.temp},
+            {"top_k", slot.sparams.top_k},
+            {"top_p", slot.sparams.top_p},
+            {"tfs_z", slot.sparams.tfs_z},
+            {"typical_p", slot.sparams.typical_p},
+            {"repeat_last_n", slot.sparams.penalty_last_n},
+            {"repeat_penalty", slot.sparams.penalty_repeat},
+            {"presence_penalty", slot.sparams.penalty_present},
             {"frequency_penalty", slot.sparams.penalty_freq},
-            {"mirostat",          slot.sparams.mirostat},
-            {"mirostat_tau",      slot.sparams.mirostat_tau},
-            {"mirostat_eta",      slot.sparams.mirostat_eta},
-            {"penalize_nl",       slot.sparams.penalize_nl},
-            {"stop",              slot.params.antiprompt},
-            {"n_predict",         slot.params.n_predict},
-            {"n_keep",            params.n_keep},
-            {"ignore_eos",        ignore_eos},
-            {"stream",            slot.params.stream},
-            {"logit_bias",        slot.sparams.logit_bias},
-            {"n_probs",           slot.sparams.n_probs},
-            {"grammar",           slot.sparams.grammar},
+            {"mirostat", slot.sparams.mirostat},
+            {"mirostat_tau", slot.sparams.mirostat_tau},
+            {"mirostat_eta", slot.sparams.mirostat_eta},
+            {"penalize_nl", slot.sparams.penalize_nl},
+            {"stop", slot.params.antiprompt},
+            {"n_predict", slot.params.n_predict},
+            {"n_keep", params.n_keep},
+            {"ignore_eos", ignore_eos},
+            {"stream", slot.params.stream},
+            {"logit_bias", slot.sparams.logit_bias},
+            {"n_probs", slot.sparams.n_probs},
+            {"grammar", slot.sparams.grammar},
         };
     }
 
@@ -1142,23 +1169,21 @@ struct llama_server_context
         res.error = false;
         res.stop = false;
 
-        res.result_json = json
-        {
-            {"content",    tkn.text_to_send},
-            {"stop",       false},
-            {"slot_id",    slot.id},
-            {"multimodal", multimodal}
-        };
+        res.result_json =
+            json{{"content", tkn.text_to_send}, {"stop", false}, {"slot_id", slot.id}, {"multimodal", multimodal}};
 
         if (slot.sparams.n_probs > 0)
         {
             std::vector<completion_token_output> probs_output = {};
             const std::vector<llama_token> to_send_toks = llama_tokenize(ctx, tkn.text_to_send, false);
             size_t probs_pos = std::min(slot.sent_token_probs_index, slot.generated_token_probs.size());
-            size_t probs_stop_pos = std::min(slot.sent_token_probs_index + to_send_toks.size(), slot.generated_token_probs.size());
+            size_t probs_stop_pos =
+                std::min(slot.sent_token_probs_index + to_send_toks.size(), slot.generated_token_probs.size());
             if (probs_pos < probs_stop_pos)
             {
-                probs_output = std::vector<completion_token_output>(slot.generated_token_probs.begin() + probs_pos, slot.generated_token_probs.begin() + probs_stop_pos);
+                probs_output =
+                    std::vector<completion_token_output>(slot.generated_token_probs.begin() + probs_pos,
+                                                         slot.generated_token_probs.begin() + probs_stop_pos);
             }
             slot.sent_token_probs_index = probs_stop_pos;
             res.result_json["completion_probabilities"] = probs_vector_to_json(ctx, probs_output);
@@ -1175,24 +1200,21 @@ struct llama_server_context
         res.error = false;
         res.stop = true;
 
-        res.result_json = json
-        {
-            {"content",             !slot.params.stream ? slot.generated_text : ""},
-            {"slot_id",             slot.id},
-            {"stop",                true},
-            {"model",               params.model_alias},
-            {"tokens_predicted",    slot.n_decoded},
-            {"tokens_evaluated",    slot.num_prompt_tokens},
-            {"generation_settings", get_formated_generation(slot)},
-            {"prompt",              slot.prompt},
-            {"truncated",           slot.truncated},
-            {"stopped_eos",         slot.stopped_eos},
-            {"stopped_word",        slot.stopped_word},
-            {"stopped_limit",       slot.stopped_limit},
-            {"stopping_word",       slot.stopping_word},
-            {"tokens_cached",       slot.n_past},
-            {"timings",             slot.get_formated_timings()}
-        };
+        res.result_json = json{{"content", !slot.params.stream ? slot.generated_text : ""},
+                               {"slot_id", slot.id},
+                               {"stop", true},
+                               {"model", params.model_alias},
+                               {"tokens_predicted", slot.n_decoded},
+                               {"tokens_evaluated", slot.num_prompt_tokens},
+                               {"generation_settings", get_formated_generation(slot)},
+                               {"prompt", slot.prompt},
+                               {"truncated", slot.truncated},
+                               {"stopped_eos", slot.stopped_eos},
+                               {"stopped_word", slot.stopped_word},
+                               {"stopped_limit", slot.stopped_limit},
+                               {"stopping_word", slot.stopping_word},
+                               {"tokens_cached", slot.n_past},
+                               {"timings", slot.get_formated_timings()}};
 
         if (slot.sparams.n_probs > 0)
         {
@@ -1200,13 +1222,14 @@ struct llama_server_context
             if (!slot.params.stream && slot.stopped_word)
             {
                 const std::vector<llama_token> stop_word_toks = llama_tokenize(ctx, slot.stopping_word, false);
-                probs = std::vector<completion_token_output>(slot.generated_token_probs.begin(), slot.generated_token_probs.end() - stop_word_toks.size());
+                probs = std::vector<completion_token_output>(slot.generated_token_probs.begin(),
+                                                             slot.generated_token_probs.end() - stop_word_toks.size());
             }
             else
             {
-                probs = std::vector<completion_token_output>(
-                                    slot.generated_token_probs.begin(),
-                                    slot.generated_token_probs.begin() + slot.sent_token_probs_index);
+                probs = std::vector<completion_token_output>(slot.generated_token_probs.begin(),
+                                                             slot.generated_token_probs.begin() +
+                                                                 slot.sent_token_probs_index);
             }
             res.result_json["completion_probabilities"] = probs_vector_to_json(ctx, probs);
         }
@@ -1228,8 +1251,7 @@ struct llama_server_context
             LOG_WARNING("embedding disabled", {
                                                   {"params.embedding", params.embedding},
                                               });
-            res.result_json = json
-            {
+            res.result_json = json{
                 {"embedding", std::vector<float>(n_embd, 0.0f)},
             };
         }
@@ -1237,9 +1259,8 @@ struct llama_server_context
         {
             const float *data = llama_get_embeddings(ctx);
             std::vector<float> embedding(data, data + n_embd);
-            res.result_json = json
-            {
-                {"embedding", embedding },
+            res.result_json = json{
+                {"embedding", embedding},
             };
         }
         queue_results.push_back(res);
@@ -1270,7 +1291,7 @@ struct llama_server_context
                 continue;
             }
 
-            for (int i = 0; i < (int) queue_results.size(); i++)
+            for (int i = 0; i < (int)queue_results.size(); i++)
             {
                 if (queue_results[i].id == task_id)
                 {
@@ -1282,7 +1303,7 @@ struct llama_server_context
         }
 
         // never reached
-        //return task_result{-1, false, false, {}};
+        // return task_result{-1, false, false, {}};
     }
 
     // for multiple images processing
@@ -1290,23 +1311,25 @@ struct llama_server_context
     {
         int image_idx = 0;
 
-        while (image_idx < (int) slot.images.size())
+        while (image_idx < (int)slot.images.size())
         {
             slot_image &img = slot.images[image_idx];
 
             // process prefix prompt
-            for (int32_t i = 0; i < (int32_t) batch.n_tokens; i += n_batch)
+            for (int32_t i = 0; i < (int32_t)batch.n_tokens; i += n_batch)
             {
-                const int32_t n_tokens = std::min(n_batch, (int32_t) (batch.n_tokens - i));
+                const int32_t n_tokens = std::min(n_batch, (int32_t)(batch.n_tokens - i));
                 llama_batch batch_view = {
                     n_tokens,
-                    batch.token    + i,
+                    batch.token + i,
                     nullptr,
-                    batch.pos      + i,
+                    batch.pos + i,
                     batch.n_seq_id + i,
-                    batch.seq_id   + i,
-                    batch.logits   + i,
-                    0, 0, 0, // unused
+                    batch.seq_id + i,
+                    batch.logits + i,
+                    0,
+                    0,
+                    0, // unused
                 };
                 if (llama_decode(ctx, batch_view))
                 {
@@ -1325,7 +1348,12 @@ struct llama_server_context
                 }
 
                 const int n_embd = llama_n_embd(model);
-                llama_batch batch_img = { n_eval, nullptr, (img.image_embedding + i * n_embd), nullptr, nullptr, nullptr, nullptr, slot.n_past, 1, 0, };
+                llama_batch batch_img = {
+                    n_eval,  nullptr,     (img.image_embedding + i * n_embd),
+                    nullptr, nullptr,     nullptr,
+                    nullptr, slot.n_past, 1,
+                    0,
+                };
                 if (llama_decode(ctx, batch_img))
                 {
                     LOG_TEE("%s : failed to eval image\n", __func__);
@@ -1338,14 +1366,15 @@ struct llama_server_context
             llama_batch_clear(batch);
 
             // append prefix of next image
-            const auto json_prompt = (image_idx >= (int) slot.images.size()) ?
-                slot.params.input_suffix : // no more images, then process suffix prompt
-                (json)(slot.images[image_idx].prefix_prompt);
+            const auto json_prompt = (image_idx >= (int)slot.images.size())
+                                       ? slot.params.input_suffix
+                                       : // no more images, then process suffix prompt
+                                         (json)(slot.images[image_idx].prefix_prompt);
 
             std::vector<llama_token> append_tokens = tokenize(json_prompt, false); // has next image
-            for (int i = 0; i < (int) append_tokens.size(); ++i)
+            for (int i = 0; i < (int)append_tokens.size(); ++i)
             {
-                llama_batch_add(batch, append_tokens[i], slot.n_past, { slot.id }, true);
+                llama_batch_add(batch, append_tokens[i], slot.n_past, {slot.id}, true);
                 slot.n_past += 1;
             }
         }
@@ -1372,49 +1401,52 @@ struct llama_server_context
             queue_tasks.erase(queue_tasks.begin());
             switch (task.type)
             {
-                case COMPLETION_TASK: {
-                    llama_client_slot *slot = get_slot(json_value(task.data, "slot_id", -1));
-                    if (slot == nullptr)
-                    {
-                        LOG_TEE("slot unavailable\n");
-                        // send error result
-                        send_error(task.id, "slot unavailable");
-                        return;
-                    }
+            case COMPLETION_TASK: {
+                llama_client_slot *slot = get_slot(json_value(task.data, "slot_id", -1));
+                if (slot == nullptr)
+                {
+                    LOG_TEE("slot unavailable\n");
+                    // send error result
+                    send_error(task.id, "slot unavailable");
+                    return;
+                }
 
-                    if (task.data.contains("system_prompt"))
-                    {
-                        process_system_prompt_data(task.data["system_prompt"]);
-                    }
+                if (task.data.contains("system_prompt"))
+                {
+                    process_system_prompt_data(task.data["system_prompt"]);
+                }
 
-                    slot->reset();
+                slot->reset();
 
-                    slot->infill = task.infill_mode;
-                    slot->embedding = task.embedding_mode;
-                    slot->task_id = task.id;
+                slot->infill = task.infill_mode;
+                slot->embedding = task.embedding_mode;
+                slot->task_id = task.id;
 
-                    if (!launch_slot_with_data(slot, task.data))
+                if (!launch_slot_with_data(slot, task.data))
+                {
+                    // send error result
+                    send_error(task.id, "internal_error");
+                    break;
+                }
+            }
+            break;
+            case CANCEL_TASK: { // release slot linked with the task id
+                for (auto &slot : slots)
+                {
+                    if (slot.task_id == task.target_id)
                     {
-                        // send error result
-                        send_error(task.id, "internal_error");
+                        slot.release();
                         break;
                     }
-                } break;
-                case CANCEL_TASK: { // release slot linked with the task id
-                    for (auto & slot : slots)
-                    {
-                        if (slot.task_id == task.target_id)
-                        {
-                            slot.release();
-                            break;
-                        }
-                    }
-                } break;
+                }
+            }
+            break;
             }
         }
     }
 
-    bool update_slots() {
+    bool update_slots()
+    {
         // attend tasks
         process_tasks();
 
@@ -1440,14 +1472,15 @@ struct llama_server_context
 
         for (llama_client_slot &slot : slots)
         {
-            if (slot.is_processing() && slot.cache_tokens.size() >= (size_t) slot.n_ctx)
+            if (slot.is_processing() && slot.cache_tokens.size() >= (size_t)slot.n_ctx)
             {
                 // Shift context
-                const int n_left    = slot.n_past - slot.params.n_keep - 1;
+                const int n_left = slot.n_past - slot.params.n_keep - 1;
                 const int n_discard = n_left / 2;
 
-                LOG_TEE("slot %d: context shift - n_keep = %d, n_left = %d, n_discard = %d\n", slot.id, slot.params.n_keep, n_left, n_discard);
-                llama_kv_cache_seq_rm   (ctx, slot.id, slot.params.n_keep + 1            , slot.params.n_keep + n_discard + 1);
+                LOG_TEE("slot %d: context shift - n_keep = %d, n_left = %d, n_discard = %d\n", slot.id,
+                        slot.params.n_keep, n_left, n_discard);
+                llama_kv_cache_seq_rm(ctx, slot.id, slot.params.n_keep + 1, slot.params.n_keep + n_discard + 1);
                 llama_kv_cache_seq_shift(ctx, slot.id, slot.params.n_keep + 1 + n_discard, slot.n_past, -n_discard);
 
                 for (size_t i = slot.params.n_keep + 1 + n_discard; i < slot.cache_tokens.size(); i++)
@@ -1462,15 +1495,15 @@ struct llama_server_context
                 slot.truncated = true;
 
                 LOG_VERBOSE("context shift", {
-                                                {"n_ctx",  n_ctx},
-                                                {"n_keep", params.n_keep},
-                                                {"n_left", n_left},
-                                            });
+                                                 {"n_ctx", n_ctx},
+                                                 {"n_keep", params.n_keep},
+                                                 {"n_left", n_left},
+                                             });
             }
         }
 
         // decode any currently ongoing sequences
-        for (auto & slot : slots)
+        for (auto &slot : slots)
         {
             // release the slot
             if (slot.command == RELEASE)
@@ -1479,7 +1512,7 @@ struct llama_server_context
                 slot.command = NONE;
                 slot.t_last_used = ggml_time_us();
 
-                LOG_TEE("slot %d released (%d tokens in cache)\n", slot.id, (int) slot.cache_tokens.size());
+                LOG_TEE("slot %d released (%d tokens in cache)\n", slot.id, (int)slot.cache_tokens.size());
 
                 continue;
             }
@@ -1491,7 +1524,7 @@ struct llama_server_context
 
             slot.i_batch = batch.n_tokens;
 
-            llama_batch_add(batch, slot.sampled, system_tokens.size() + slot.n_past, { slot.id }, true);
+            llama_batch_add(batch, slot.sampled, system_tokens.size() + slot.n_past, {slot.id}, true);
 
             slot.n_decoded += 1;
             slot.n_past += 1;
@@ -1503,9 +1536,11 @@ struct llama_server_context
         // assign workload to the slots
         if (params.cont_batching || batch.n_tokens == 0)
         {
-            for (auto & slot : slots)
+            for (auto &slot : slots)
             {
-                const bool has_prompt = slot.prompt.is_array() || (slot.prompt.is_string() && !slot.prompt.get<std::string>().empty()) || !slot.images.empty();
+                const bool has_prompt = slot.prompt.is_array() ||
+                                        (slot.prompt.is_string() && !slot.prompt.get<std::string>().empty()) ||
+                                        !slot.images.empty();
 
                 // empty prompt passed -> release the slot and send empty response
                 if (slot.state == IDLE && slot.command == LOAD_PROMPT && !has_prompt)
@@ -1537,7 +1572,8 @@ struct llama_server_context
                         auto suffix_tokens = tokenize(slot.params.input_suffix, false);
 
                         const int space_token = 29871; // TODO: this should not be hardcoded
-                        if (suff_rm_leading_spc && !suffix_tokens.empty() && suffix_tokens[0] == space_token) {
+                        if (suff_rm_leading_spc && !suffix_tokens.empty() && suffix_tokens[0] == space_token)
+                        {
                             suffix_tokens.erase(suffix_tokens.begin());
                         }
 
@@ -1550,7 +1586,8 @@ struct llama_server_context
                     }
                     else
                     {
-                        prompt_tokens = tokenize(slot.prompt, system_prompt.empty());  // add BOS if there isn't system prompt
+                        prompt_tokens =
+                            tokenize(slot.prompt, system_prompt.empty()); // add BOS if there isn't system prompt
                     }
 
                     slot.num_prompt_tokens = prompt_tokens.size();
@@ -1575,17 +1612,22 @@ struct llama_server_context
                         {
                             const int n_left = slot.n_ctx - slot.params.n_keep;
                             const int n_block_size = n_left / 2;
-                            const int erased_blocks = (slot.num_prompt_tokens - slot.params.n_keep - n_block_size) / n_block_size;
-
-                            std::vector<llama_token> new_tokens(prompt_tokens.begin(), prompt_tokens.begin() + slot.params.n_keep);
-                            new_tokens.insert(new_tokens.end(), prompt_tokens.begin() + slot.params.n_keep + erased_blocks * n_block_size, prompt_tokens.end());
-
-                            LOG_VERBOSE("input truncated", {
-                                                            {"n_ctx",  slot.n_ctx},
-                                                            {"n_keep", slot.params.n_keep},
-                                                            {"n_left", n_left},
-                                                            {"new_tokens", tokens_to_str(ctx, new_tokens.cbegin(), new_tokens.cend())},
-                                                        });
+                            const int erased_blocks =
+                                (slot.num_prompt_tokens - slot.params.n_keep - n_block_size) / n_block_size;
+
+                            std::vector<llama_token> new_tokens(prompt_tokens.begin(),
+                                                                prompt_tokens.begin() + slot.params.n_keep);
+                            new_tokens.insert(new_tokens.end(),
+                                              prompt_tokens.begin() + slot.params.n_keep + erased_blocks * n_block_size,
+                                              prompt_tokens.end());
+
+                            LOG_VERBOSE("input truncated",
+                                        {
+                                            {"n_ctx", slot.n_ctx},
+                                            {"n_keep", slot.params.n_keep},
+                                            {"n_left", n_left},
+                                            {"new_tokens", tokens_to_str(ctx, new_tokens.cbegin(), new_tokens.cend())},
+                                        });
                             slot.truncated = true;
                             prompt_tokens = new_tokens;
 
@@ -1602,10 +1644,11 @@ struct llama_server_context
                         slot.n_past = common_part(slot.cache_tokens, prompt_tokens);
                         slot.num_prompt_tokens_processed = slot.num_prompt_tokens - slot.n_past;
 
-                        LOG_TEE("slot %d : in cache: %i tokens | to process: %i tokens\n", slot.id, slot.n_past, slot.num_prompt_tokens_processed);
+                        LOG_TEE("slot %d : in cache: %i tokens | to process: %i tokens\n", slot.id, slot.n_past,
+                                slot.num_prompt_tokens_processed);
                     }
 
-                    LOG_TEE("slot %d : kv cache rm - [%d, end)\n", slot.id, (int) system_tokens.size() + slot.n_past);
+                    LOG_TEE("slot %d : kv cache rm - [%d, end)\n", slot.id, (int)system_tokens.size() + slot.n_past);
 
                     llama_kv_cache_seq_rm(ctx, slot.id, system_tokens.size() + slot.n_past, -1);
 
@@ -1618,19 +1661,24 @@ struct llama_server_context
                         slot.n_past--;
                     }
 
-                    LOG_VERBOSE("prompt ingested", {
-                                                    {"n_past", slot.n_past},
-                                                    {"cached", tokens_to_str(ctx, slot.cache_tokens.cbegin(), slot.cache_tokens.cbegin() + slot.n_past)},
-                                                    {"to_eval", tokens_to_str(ctx, slot.cache_tokens.cbegin() + slot.n_past, slot.cache_tokens.cend())},
-                                                });
+                    LOG_VERBOSE("prompt ingested",
+                                {
+                                    {"n_past", slot.n_past},
+                                    {"cached", tokens_to_str(ctx, slot.cache_tokens.cbegin(),
+                                                             slot.cache_tokens.cbegin() + slot.n_past)},
+                                    {"to_eval", tokens_to_str(ctx, slot.cache_tokens.cbegin() + slot.n_past,
+                                                              slot.cache_tokens.cend())},
+                                });
 
                     const bool has_images = process_images(slot);
 
                     // process the prefix of first image
-                    std::vector<llama_token> prefix_tokens = has_images ? tokenize(slot.images[0].prefix_prompt, true) : prompt_tokens;
-                    for (; slot.n_past < (int) prefix_tokens.size(); ++slot.n_past)
+                    std::vector<llama_token> prefix_tokens =
+                        has_images ? tokenize(slot.images[0].prefix_prompt, true) : prompt_tokens;
+                    for (; slot.n_past < (int)prefix_tokens.size(); ++slot.n_past)
                     {
-                       llama_batch_add(batch, prefix_tokens[slot.n_past], system_tokens.size() + slot.n_past, { slot.id }, false);
+                        llama_batch_add(batch, prefix_tokens[slot.n_past], system_tokens.size() + slot.n_past,
+                                        {slot.id}, false);
                     }
 
                     if (has_images && !ingest_images(slot, n_batch))
@@ -1646,7 +1694,7 @@ struct llama_server_context
                     }
 
                     slot.n_decoded = 0;
-                    slot.i_batch   = batch.n_tokens - 1;
+                    slot.i_batch = batch.n_tokens - 1;
                 }
             }
         }
@@ -1657,19 +1705,20 @@ struct llama_server_context
             return true;
         }
 
-        for (int32_t i = 0; i < (int32_t) batch.n_tokens; i += n_batch)
+        for (int32_t i = 0; i < (int32_t)batch.n_tokens; i += n_batch)
         {
-            const int32_t n_tokens = std::min(n_batch, (int32_t) (batch.n_tokens - i));
-            llama_batch batch_view =
-            {
+            const int32_t n_tokens = std::min(n_batch, (int32_t)(batch.n_tokens - i));
+            llama_batch batch_view = {
                 n_tokens,
-                batch.token    + i,
+                batch.token + i,
                 nullptr,
-                batch.pos      + i,
+                batch.pos + i,
                 batch.n_seq_id + i,
-                batch.seq_id   + i,
-                batch.logits   + i,
-                0, 0, 0, // unused
+                batch.seq_id + i,
+                batch.logits + i,
+                0,
+                0,
+                0, // unused
             };
 
             const int ret = llama_decode(ctx, batch_view);
@@ -1682,7 +1731,8 @@ struct llama_server_context
                     return false;
                 }
 
-                LOG_TEE("%s : failed to find free space in the KV cache, retrying with smaller n_batch = %d\n", __func__, n_batch / 2);
+                LOG_TEE("%s : failed to find free space in the KV cache, retrying with smaller n_batch = %d\n",
+                        __func__, n_batch / 2);
 
                 // retry with half the batch size to try to find a free slot in the KV cache
                 n_batch /= 2;
@@ -1690,9 +1740,9 @@ struct llama_server_context
                 continue;
             }
 
-            for (auto & slot : slots)
+            for (auto &slot : slots)
             {
-                if (slot.i_batch < (int) i || slot.i_batch >= (int) (i + n_tokens))
+                if (slot.i_batch < (int)i || slot.i_batch >= (int)(i + n_tokens))
                 {
                     continue;
                 }
@@ -1717,7 +1767,7 @@ struct llama_server_context
                     slot.t_prompt_processing = (slot.t_start_genereration - slot.t_start_process_prompt) / 1e3;
                 }
 
-                llama_token_data_array cur_p = { slot.ctx_sampling->cur.data(), slot.ctx_sampling->cur.size(), false };
+                llama_token_data_array cur_p = {slot.ctx_sampling->cur.data(), slot.ctx_sampling->cur.size(), false};
                 result.tok = id;
 
                 const int32_t n_probs = slot.sparams.n_probs;
@@ -1746,8 +1796,7 @@ struct llama_server_context
     }
 };
 
-static void server_print_usage(const char *argv0, const gpt_params &params,
-                               const server_params &sparams)
+static void server_print_usage(const char *argv0, const gpt_params &params, const server_params &sparams)
 {
     printf("usage: %s [options]\n", argv0);
     printf("\n");
@@ -1755,10 +1804,12 @@ static void server_print_usage(const char *argv0, const gpt_params &params,
     printf("  -h, --help                show this help message and exit\n");
     printf("  -v, --verbose             verbose output (default: %s)\n", server_verbose ? "enabled" : "disabled");
     printf("  -t N, --threads N         number of threads to use during computation (default: %d)\n", params.n_threads);
-    printf("  -tb N, --threads-batch N  number of threads to use during batch and prompt processing (default: same as --threads)\n");
+    printf("  -tb N, --threads-batch N  number of threads to use during batch and prompt processing (default: same as "
+           "--threads)\n");
     printf("  -c N, --ctx-size N        size of the prompt context (default: %d)\n", params.n_ctx);
     printf("  --rope-scaling {none,linear,yarn}\n");
-    printf("                            RoPE frequency scaling method, defaults to linear unless specified by the model\n");
+    printf("                            RoPE frequency scaling method, defaults to linear unless specified by the "
+           "model\n");
     printf("  --rope-freq-base N        RoPE base frequency (default: loaded from model)\n");
     printf("  --rope-freq-scale N       RoPE frequency scaling factor, expands context by a factor of 1/N\n");
     printf("  --yarn-ext-factor N       YaRN: extrapolation mix factor (default: 1.0, 0.0 = full interpolation)\n");
@@ -1767,21 +1818,24 @@ static void server_print_usage(const char *argv0, const gpt_params &params,
     printf("  --yarn-beta-fast N        YaRN: low correction dim or beta (default: %.1f)\n", params.yarn_beta_fast);
     printf("  -b N, --batch-size N      batch size for prompt processing (default: %d)\n", params.n_batch);
     printf("  --memory-f32              use f32 instead of f16 for memory key+value (default: disabled)\n");
-    printf("                            not recommended: doubles context memory required and no measurable increase in quality\n");
+    printf("                            not recommended: doubles context memory required and no measurable increase in "
+           "quality\n");
     if (llama_mlock_supported())
     {
         printf("  --mlock               force system to keep model in RAM rather than swapping or compressing\n");
     }
     if (llama_mmap_supported())
     {
-        printf("  --no-mmap             do not memory-map model (slower load but may reduce pageouts if not using mlock)\n");
+        printf("  --no-mmap             do not memory-map model (slower load but may reduce pageouts if not using "
+               "mlock)\n");
     }
     printf("  --numa                attempt optimizations that help on some NUMA systems\n");
 #ifdef LLAMA_SUPPORTS_GPU_OFFLOAD
     printf("  -ngl N, --n-gpu-layers N\n");
     printf("                        number of layers to store in VRAM\n");
     printf("  -ts SPLIT --tensor-split SPLIT\n");
-    printf("                        how to split tensors across multiple GPUs, comma-separated list of proportions, e.g. 3,1\n");
+    printf("                        how to split tensors across multiple GPUs, comma-separated list of proportions, "
+           "e.g. 3,1\n");
     printf("  -mg i, --main-gpu i   the GPU to use for scratch and small tensors\n");
     printf("  -nommq, --no-mul-mat-q\n");
     printf("                        use cuBLAS instead of custom mul_mat_q CUDA kernels.\n");
@@ -1790,24 +1844,27 @@ static void server_print_usage(const char *argv0, const gpt_params &params,
     printf("  -m FNAME, --model FNAME\n");
     printf("                        model path (default: %s)\n", params.model.c_str());
     printf("  -a ALIAS, --alias ALIAS\n");
-    printf("                        set an alias for the model, will be added as `model` field in completion response\n");
+    printf(
+        "                        set an alias for the model, will be added as `model` field in completion response\n");
     printf("  --lora FNAME          apply LoRA adapter (implies --no-mmap)\n");
     printf("  --lora-base FNAME     optional model to use as a base for the layers modified by the LoRA adapter\n");
     printf("  --host                ip address to listen (default  (default: %s)\n", sparams.hostname.c_str());
     printf("  --port PORT           port to listen (default  (default: %d)\n", sparams.port);
     printf("  --path PUBLIC_PATH    path from which to serve static files (default %s)\n", sparams.public_path.c_str());
     printf("  -to N, --timeout N    server read/write timeout in seconds (default: %d)\n", sparams.read_timeout);
-    printf("  --embedding           enable embedding vector output (default: %s)\n", params.embedding ? "enabled" : "disabled");
+    printf("  --embedding           enable embedding vector output (default: %s)\n",
+           params.embedding ? "enabled" : "disabled");
     printf("  -np N, --parallel N   number of slots for process requests (default: %d)\n", params.n_parallel);
     printf("  -cb, --cont-batching  enable continuous batching (a.k.a dynamic batching) (default: disabled)\n");
     printf("    -spf FNAME, --system-prompt-file FNAME\n");
-    printf("                        Set a file to load a system prompt (initial prompt of all slots), this is useful for chat applications.\n");
+    printf("                        Set a file to load a system prompt (initial prompt of all slots), this is useful "
+           "for chat applications.\n");
     printf("  --mmproj MMPROJ_FILE  path to a multimodal projector file for LLaVA.\n");
     printf("\n");
 }
 
-static void server_params_parse(int argc, char **argv, server_params &sparams,
-                                gpt_params &params, llama_server_context& llama)
+static void server_params_parse(int argc, char **argv, server_params &sparams, gpt_params &params,
+                                llama_server_context &llama)
 {
     gpt_params default_params;
     server_params default_sparams;
@@ -1894,10 +1951,23 @@ static void server_params_parse(int argc, char **argv, server_params &sparams,
                 break;
             }
             std::string value(argv[i]);
-            /**/ if (value == "none")   { params.rope_scaling_type = LLAMA_ROPE_SCALING_NONE; }
-            else if (value == "linear") { params.rope_scaling_type = LLAMA_ROPE_SCALING_LINEAR; }
-            else if (value == "yarn")   { params.rope_scaling_type = LLAMA_ROPE_SCALING_YARN; }
-            else { invalid_param = true; break; }
+            /**/ if (value == "none")
+            {
+                params.rope_scaling_type = LLAMA_ROPE_SCALING_NONE;
+            }
+            else if (value == "linear")
+            {
+                params.rope_scaling_type = LLAMA_ROPE_SCALING_LINEAR;
+            }
+            else if (value == "yarn")
+            {
+                params.rope_scaling_type = LLAMA_ROPE_SCALING_YARN;
+            }
+            else
+            {
+                invalid_param = true;
+                break;
+            }
         }
         else if (arg == "--rope-freq-base")
         {
@@ -1919,7 +1989,8 @@ static void server_params_parse(int argc, char **argv, server_params &sparams,
         }
         else if (arg == "--yarn-ext-factor")
         {
-            if (++i >= argc) {
+            if (++i >= argc)
+            {
                 invalid_param = true;
                 break;
             }
@@ -1927,7 +1998,8 @@ static void server_params_parse(int argc, char **argv, server_params &sparams,
         }
         else if (arg == "--yarn-attn-factor")
         {
-            if (++i >= argc) {
+            if (++i >= argc)
+            {
                 invalid_param = true;
                 break;
             }
@@ -1935,7 +2007,8 @@ static void server_params_parse(int argc, char **argv, server_params &sparams,
         }
         else if (arg == "--yarn-beta-fast")
         {
-            if (++i >= argc) {
+            if (++i >= argc)
+            {
                 invalid_param = true;
                 break;
             }
@@ -1943,7 +2016,8 @@ static void server_params_parse(int argc, char **argv, server_params &sparams,
         }
         else if (arg == "--yarn-beta-slow")
         {
-            if (++i >= argc) {
+            if (++i >= argc)
+            {
                 invalid_param = true;
                 break;
             }
@@ -2032,7 +2106,8 @@ static void server_params_parse(int argc, char **argv, server_params &sparams,
 #ifdef GGML_USE_CUBLAS
             params.mul_mat_q = false;
 #else
-            LOG_WARNING("warning: llama.cpp was compiled without cuBLAS. Disabling mul_mat_q kernels has no effect.\n", {});
+            LOG_WARNING("warning: llama.cpp was compiled without cuBLAS. Disabling mul_mat_q kernels has no effect.\n",
+                        {});
 #endif // GGML_USE_CUBLAS
         }
         else if (arg == "--main-gpu" || arg == "-mg")
@@ -2065,7 +2140,7 @@ static void server_params_parse(int argc, char **argv, server_params &sparams,
                 invalid_param = true;
                 break;
             }
-            const char * lora_adapter = argv[i];
+            const char *lora_adapter = argv[i];
             if (++i >= argc)
             {
                 invalid_param = true;
@@ -2119,7 +2194,8 @@ static void server_params_parse(int argc, char **argv, server_params &sparams,
                 break;
             }
             params.n_parallel = std::stoi(argv[i]);
-        } else if (arg == "-n" || arg == "--n-predict")
+        }
+        else if (arg == "-n" || arg == "--n-predict")
         {
             if (++i >= argc)
             {
@@ -2127,7 +2203,8 @@ static void server_params_parse(int argc, char **argv, server_params &sparams,
                 break;
             }
             params.n_predict = std::stoi(argv[i]);
-        } else if (arg == "-spf" || arg == "--system-prompt-file")
+        }
+        else if (arg == "-spf" || arg == "--system-prompt-file")
         {
             if (++i >= argc)
             {
@@ -2135,20 +2212,18 @@ static void server_params_parse(int argc, char **argv, server_params &sparams,
                 break;
             }
             std::ifstream file(argv[i]);
-            if (!file) {
+            if (!file)
+            {
                 fprintf(stderr, "error: failed to open file '%s'\n", argv[i]);
                 invalid_param = true;
                 break;
             }
             std::string systm_content;
-            std::copy(
-                std::istreambuf_iterator<char>(file),
-                std::istreambuf_iterator<char>(),
-                std::back_inserter(systm_content)
-            );
+            std::copy(std::istreambuf_iterator<char>(file), std::istreambuf_iterator<char>(),
+                      std::back_inserter(systm_content));
             llama.process_system_prompt_data(json::parse(systm_content));
         }
-        else if(arg == "--mmproj")
+        else if (arg == "--mmproj")
         {
             if (++i >= argc)
             {
@@ -2173,16 +2248,10 @@ static void server_params_parse(int argc, char **argv, server_params &sparams,
     }
 }
 
-static json format_partial_response(
-    llama_server_context &llama, llama_client_slot *slot, const std::string &content, const std::vector<completion_token_output> &probs
-) {
-    json res = json
-    {
-        {"content",    content },
-        {"stop",       false},
-        {"slot_id",    slot->id },
-        {"multimodal", llama.multimodal }
-    };
+static json format_partial_response(llama_server_context &llama, llama_client_slot *slot, const std::string &content,
+                                    const std::vector<completion_token_output> &probs)
+{
+    json res = json{{"content", content}, {"stop", false}, {"slot_id", slot->id}, {"multimodal", llama.multimodal}};
 
     if (slot->sparams.n_probs > 0)
     {
@@ -2194,17 +2263,14 @@ static json format_partial_response(
 
 static json format_tokenizer_response(const std::vector<llama_token> &tokens)
 {
-    return json{
-        {"tokens", tokens}};
+    return json{{"tokens", tokens}};
 }
 
 static json format_detokenized_response(std::string content)
 {
-    return json{
-        {"content", content}};
+    return json{{"content", content}};
 }
 
-
 static void log_server_request(const httplib::Request &req, const httplib::Response &res)
 {
     LOG_INFO("request", {
@@ -2224,22 +2290,28 @@ static void log_server_request(const httplib::Request &req, const httplib::Respo
 
 struct token_translator
 {
-    llama_context * ctx;
-    std::string operator()(llama_token tok)                    const { return llama_token_to_piece(ctx, tok); }
-    std::string operator()(const completion_token_output &cto) const { return (*this)(cto.tok); }
+    llama_context *ctx;
+    std::string operator()(llama_token tok) const
+    {
+        return llama_token_to_piece(ctx, tok);
+    }
+    std::string operator()(const completion_token_output &cto) const
+    {
+        return (*this)(cto.tok);
+    }
 };
 
 static void append_to_generated_text_from_generated_token_probs(llama_server_context &llama, llama_client_slot *slot)
 {
-    auto & gtps = slot->generated_token_probs;
+    auto &gtps = slot->generated_token_probs;
     auto translator = token_translator{llama.ctx};
-    auto add_strlen = [=](size_t sum, const completion_token_output & cto) { return sum + translator(cto).size(); };
+    auto add_strlen = [=](size_t sum, const completion_token_output &cto) { return sum + translator(cto).size(); };
     const size_t len = std::accumulate(gtps.begin(), gtps.end(), size_t(0), add_strlen);
     if (slot->generated_text.capacity() < slot->generated_text.size() + len)
     {
         slot->generated_text.reserve(slot->generated_text.size() + len);
     }
-    for (const completion_token_output & cto : gtps)
+    for (const completion_token_output &cto : gtps)
     {
         slot->generated_text += translator(cto);
     }
@@ -2263,8 +2335,7 @@ int main(int argc, char **argv)
 
     llama_backend_init(params.numa);
 
-    LOG_INFO("build info", {{"build", LLAMA_BUILD_NUMBER},
-                            {"commit", LLAMA_COMMIT}});
+    LOG_INFO("build info", {{"build", LLAMA_BUILD_NUMBER}, {"commit", LLAMA_COMMIT}});
 
     LOG_INFO("system info", {
                                 {"n_threads", params.n_threads},
@@ -2288,252 +2359,242 @@ int main(int argc, char **argv)
                              {"Access-Control-Allow-Headers", "content-type"}});
 
     // this is only called if no index.html is found in the public --path
-    svr.Get("/", [](const httplib::Request &, httplib::Response &res)
-            {
-                res.set_content(reinterpret_cast<const char*>(&index_html), index_html_len, "text/html");
-                return false;
-            });
+    svr.Get("/", [](const httplib::Request &, httplib::Response &res) {
+        res.set_content(reinterpret_cast<const char *>(&index_html), index_html_len, "text/html");
+        return false;
+    });
 
     // this is only called if no index.js is found in the public --path
-    svr.Get("/index.js", [](const httplib::Request &, httplib::Response &res)
-            {
-                res.set_content(reinterpret_cast<const char *>(&index_js), index_js_len, "text/javascript");
-                return false;
-            });
+    svr.Get("/index.js", [](const httplib::Request &, httplib::Response &res) {
+        res.set_content(reinterpret_cast<const char *>(&index_js), index_js_len, "text/javascript");
+        return false;
+    });
 
     // this is only called if no index.html is found in the public --path
-    svr.Get("/completion.js", [](const httplib::Request &, httplib::Response &res)
-            {
-                res.set_content(reinterpret_cast<const char*>(&completion_js), completion_js_len, "application/javascript");
-                return false;
-            });
+    svr.Get("/completion.js", [](const httplib::Request &, httplib::Response &res) {
+        res.set_content(reinterpret_cast<const char *>(&completion_js), completion_js_len, "application/javascript");
+        return false;
+    });
 
     // this is only called if no index.html is found in the public --path
-    svr.Get("/json-schema-to-grammar.mjs", [](const httplib::Request &, httplib::Response &res)
-            {
-                res.set_content(reinterpret_cast<const char*>(&json_schema_to_grammar_mjs), json_schema_to_grammar_mjs_len, "application/javascript");
-                return false;
-            });
-
-    svr.Get("/props", [&llama](const httplib::Request & /*req*/, httplib::Response &res)
-            {
-                res.set_header("Access-Control-Allow-Origin", "*");
-                json data = {
-                    { "user_name",      llama.name_user.c_str() },
-                    { "assistant_name", llama.name_assistant.c_str() }
-                };
-                res.set_content(data.dump(), "application/json");
-            });
-
-    svr.Post("/completion", [&llama](const httplib::Request &req, httplib::Response &res)
+    svr.Get("/json-schema-to-grammar.mjs", [](const httplib::Request &, httplib::Response &res) {
+        res.set_content(reinterpret_cast<const char *>(&json_schema_to_grammar_mjs), json_schema_to_grammar_mjs_len,
+                        "application/javascript");
+        return false;
+    });
+
+    svr.Get("/props", [&llama](const httplib::Request & /*req*/, httplib::Response &res) {
+        res.set_header("Access-Control-Allow-Origin", "*");
+        json data = {{"user_name", llama.name_user.c_str()}, {"assistant_name", llama.name_assistant.c_str()}};
+        res.set_content(data.dump(), "application/json");
+    });
+
+    svr.Post("/completion", [&llama](const httplib::Request &req, httplib::Response &res) {
+        json data = json::parse(req.body);
+        const int task_id = llama.request_completion(data, false, false);
+        if (!json_value(data, "stream", false))
+        {
+            std::string completion_text;
+            task_result result = llama.next_result(task_id);
+            if (!result.error && result.stop)
+            {
+                res.set_content(result.result_json.dump(-1, ' ', false, json::error_handler_t::replace),
+                                "application/json");
+            }
+            else
             {
-                json data = json::parse(req.body);
-                const int task_id = llama.request_completion(data, false, false);
-                if (!json_value(data, "stream", false)) {
-                    std::string completion_text;
+                res.status = 404;
+                res.set_content(result.result_json["content"], "text/plain");
+                return;
+            }
+        }
+        else
+        {
+            const auto chunked_content_provider = [task_id, &llama](size_t, httplib::DataSink &sink) {
+                while (true)
+                {
                     task_result result = llama.next_result(task_id);
-                    if (!result.error && result.stop) {
-                        res.set_content(result.result_json.dump(-1, ' ', false, json::error_handler_t::replace), "application/json");
+                    if (!result.error)
+                    {
+                        const std::string str =
+                            "data: " + result.result_json.dump(-1, ' ', false, json::error_handler_t::replace) + "\n\n";
+                        LOG_VERBOSE("data stream", {{"to_send", str}});
+                        if (!sink.write(str.c_str(), str.size()))
+                        {
+                            return false;
+                        }
+                        if (result.stop)
+                        {
+                            break;
+                        }
                     }
                     else
                     {
-                        res.status = 404;
-                        res.set_content(result.result_json["content"], "text/plain");
-                        return;
+                        break;
                     }
-                } else {
-                    const auto chunked_content_provider = [task_id, &llama](size_t, httplib::DataSink & sink)
-                    {
-                        while (true)
-                        {
-                            task_result result = llama.next_result(task_id);
-                            if (!result.error) {
-                                const std::string str =
-                                "data: " +
-                                result.result_json.dump(-1, ' ', false, json::error_handler_t::replace) +
-                                "\n\n";
-                                LOG_VERBOSE("data stream", {
-                                    { "to_send", str }
-                                });
-                                if (!sink.write(str.c_str(), str.size()))
-                                {
-                                    return false;
-                                }
-                                if (result.stop) {
-                                    break;
-                                }
-                            } else {
-                                break;
-                            }
-                        }
-                        sink.done();
-                        return true;
-                    };
+                }
+                sink.done();
+                return true;
+            };
 
-                    auto on_complete = [task_id, &llama] (bool)
-                    {
-                        // cancel
-                        llama.request_cancel(task_id);
-                    };
+            auto on_complete = [task_id, &llama](bool) {
+                // cancel
+                llama.request_cancel(task_id);
+            };
 
-                    res.set_chunked_content_provider("text/event-stream", chunked_content_provider, on_complete);
-                }
-            });
+            res.set_chunked_content_provider("text/event-stream", chunked_content_provider, on_complete);
+        }
+    });
 
-    svr.Post("/infill", [&llama](const httplib::Request &req, httplib::Response &res)
+    svr.Post("/infill", [&llama](const httplib::Request &req, httplib::Response &res) {
+        json data = json::parse(req.body);
+        const int task_id = llama.request_completion(data, true, false);
+        if (!json_value(data, "stream", false))
+        {
+            std::string completion_text;
+            task_result result = llama.next_result(task_id);
+            if (!result.error && result.stop)
             {
-                json data = json::parse(req.body);
-                const int task_id = llama.request_completion(data, true, false);
-                if (!json_value(data, "stream", false)) {
-                    std::string completion_text;
+                res.set_content(result.result_json.dump(-1, ' ', false, json::error_handler_t::replace),
+                                "application/json");
+            }
+            else
+            {
+                res.status = 404;
+                res.set_content(result.result_json["content"], "text/plain");
+                return;
+            }
+        }
+        else
+        {
+            const auto chunked_content_provider = [task_id, &llama](size_t, httplib::DataSink &sink) {
+                while (true)
+                {
                     task_result result = llama.next_result(task_id);
-                    if (!result.error && result.stop)
+                    if (!result.error)
                     {
-                        res.set_content(result.result_json.dump(-1, ' ', false, json::error_handler_t::replace), "application/json");
+                        const std::string str =
+                            "data: " + result.result_json.dump(-1, ' ', false, json::error_handler_t::replace) + "\n\n";
+                        LOG_VERBOSE("data stream", {{"to_send", str}});
+                        if (!sink.write(str.c_str(), str.size()))
+                        {
+                            return false;
+                        }
+                        if (result.stop)
+                        {
+                            break;
+                        }
                     }
                     else
                     {
-                        res.status = 404;
-                        res.set_content(result.result_json["content"], "text/plain");
-                        return;
+                        break;
                     }
-                } else {
-                    const auto chunked_content_provider = [task_id, &llama](size_t, httplib::DataSink & sink) {
-                        while (true)
-                        {
-                            task_result result = llama.next_result(task_id);
-                            if (!result.error) {
-                                const std::string str =
-                                "data: " +
-                                result.result_json.dump(-1, ' ', false, json::error_handler_t::replace) +
-                                "\n\n";
-                                LOG_VERBOSE("data stream", {
-                                    { "to_send", str }
-                                });
-                                if (!sink.write(str.c_str(), str.size()))
-                                {
-                                    return false;
-                                }
-                                if (result.stop)
-                                {
-                                    break;
-                                }
-                            }
-                            else
-                            {
-                                break;
-                            }
-                        }
+                }
 
-                        sink.done();
+                sink.done();
 
-                        return true;
-                    };
+                return true;
+            };
 
-                    auto on_complete = [task_id, &llama] (bool)
-                    {
-                        // cancel
-                        llama.request_cancel(task_id);
-                    };
+            auto on_complete = [task_id, &llama](bool) {
+                // cancel
+                llama.request_cancel(task_id);
+            };
 
-                    res.set_chunked_content_provider("text/event-stream", chunked_content_provider, on_complete);
-                }
-            });
+            res.set_chunked_content_provider("text/event-stream", chunked_content_provider, on_complete);
+        }
+    });
 
-    svr.Get("/model.json", [&llama](const httplib::Request &, httplib::Response &res)
-            {
-                const json data = llama.get_model_props();
-                return res.set_content(data.dump(), "application/json");
-            });
+    svr.Get("/model.json", [&llama](const httplib::Request &, httplib::Response &res) {
+        const json data = llama.get_model_props();
+        return res.set_content(data.dump(), "application/json");
+    });
 
-    svr.Options(R"(/.*)", [](const httplib::Request &, httplib::Response &res)
-                { return res.set_content("", "application/json"); });
+    svr.Options(R"(/.*)", [](const httplib::Request &, httplib::Response &res) {
+        return res.set_content("", "application/json");
+    });
 
-    svr.Post("/tokenize", [&llama](const httplib::Request &req, httplib::Response &res)
-            {
-                const json body = json::parse(req.body);
-                std::vector<llama_token> tokens;
-                if (body.count("content") != 0)
-                {
-                    tokens = llama.tokenize(body["content"], false);
-                }
-                const json data = format_tokenizer_response(tokens);
-                return res.set_content(data.dump(), "application/json");
-            });
+    svr.Post("/tokenize", [&llama](const httplib::Request &req, httplib::Response &res) {
+        const json body = json::parse(req.body);
+        std::vector<llama_token> tokens;
+        if (body.count("content") != 0)
+        {
+            tokens = llama.tokenize(body["content"], false);
+        }
+        const json data = format_tokenizer_response(tokens);
+        return res.set_content(data.dump(), "application/json");
+    });
 
-    svr.Post("/detokenize", [&llama](const httplib::Request &req, httplib::Response &res)
-            {
-                const json body = json::parse(req.body);
-                std::string content;
-                if (body.count("tokens") != 0)
-                {
-                    const std::vector<llama_token> tokens = body["tokens"];
-                    content = tokens_to_str(llama.ctx, tokens.cbegin(), tokens.cend());
-                }
+    svr.Post("/detokenize", [&llama](const httplib::Request &req, httplib::Response &res) {
+        const json body = json::parse(req.body);
+        std::string content;
+        if (body.count("tokens") != 0)
+        {
+            const std::vector<llama_token> tokens = body["tokens"];
+            content = tokens_to_str(llama.ctx, tokens.cbegin(), tokens.cend());
+        }
 
-                const json data = format_detokenized_response(content);
-                return res.set_content(data.dump(), "application/json");
-            });
+        const json data = format_detokenized_response(content);
+        return res.set_content(data.dump(), "application/json");
+    });
 
-    svr.Post("/embedding", [&llama](const httplib::Request &req, httplib::Response &res)
-            {
-                const json body = json::parse(req.body);
-                json prompt;
-                if (body.count("content") != 0)
-                {
-                    prompt = body["content"];
-                }
-                else
-                {
-                    prompt = "";
-                }
-                const int task_id = llama.request_completion({ {"prompt", prompt}, { "n_predict", 0} }, false, true);
-                task_result result = llama.next_result(task_id);
-                return res.set_content(result.result_json.dump(), "application/json");
-            });
+    svr.Post("/embedding", [&llama](const httplib::Request &req, httplib::Response &res) {
+        const json body = json::parse(req.body);
+        json prompt;
+        if (body.count("content") != 0)
+        {
+            prompt = body["content"];
+        }
+        else
+        {
+            prompt = "";
+        }
+        const int task_id = llama.request_completion({{"prompt", prompt}, {"n_predict", 0}}, false, true);
+        task_result result = llama.next_result(task_id);
+        return res.set_content(result.result_json.dump(), "application/json");
+    });
 
     svr.set_logger(log_server_request);
 
-    svr.set_exception_handler([](const httplib::Request &, httplib::Response &res, std::exception_ptr ep)
-            {
-                const char fmt[] = "500 Internal Server Error\n%s";
-                char buf[BUFSIZ];
-                try
-                {
-                    std::rethrow_exception(std::move(ep));
-                }
-                catch (std::exception &e)
-                {
-                    snprintf(buf, sizeof(buf), fmt, e.what());
-                }
-                catch (...)
-                {
-                    snprintf(buf, sizeof(buf), fmt, "Unknown Exception");
-                }
-                res.set_content(buf, "text/plain");
-                res.status = 500;
-            });
+    svr.set_exception_handler([](const httplib::Request &, httplib::Response &res, std::exception_ptr ep) {
+        const char fmt[] = "500 Internal Server Error\n%s";
+        char buf[BUFSIZ];
+        try
+        {
+            std::rethrow_exception(std::move(ep));
+        }
+        catch (std::exception &e)
+        {
+            snprintf(buf, sizeof(buf), fmt, e.what());
+        }
+        catch (...)
+        {
+            snprintf(buf, sizeof(buf), fmt, "Unknown Exception");
+        }
+        res.set_content(buf, "text/plain");
+        res.status = 500;
+    });
 
-    svr.set_error_handler([](const httplib::Request &, httplib::Response &res)
-            {
-                if (res.status == 400)
-                {
-                    res.set_content("Invalid request", "text/plain");
-                }
-                else if (res.status != 500)
-                {
-                    res.set_content("File Not Found", "text/plain");
-                    res.status = 404;
-                }
-            });
+    svr.set_error_handler([](const httplib::Request &, httplib::Response &res) {
+        if (res.status == 400)
+        {
+            res.set_content("Invalid request", "text/plain");
+        }
+        else if (res.status != 500)
+        {
+            res.set_content("File Not Found", "text/plain");
+            res.status = 404;
+        }
+    });
 
     // set timeouts and change hostname and port
-    svr.set_read_timeout (sparams.read_timeout);
+    svr.set_read_timeout(sparams.read_timeout);
     svr.set_write_timeout(sparams.write_timeout);
 
     if (!svr.bind_to_port(sparams.hostname, sparams.port))
     {
-        fprintf(stderr, "\ncouldn't bind to server socket: hostname=%s port=%d\n\n", sparams.hostname.c_str(), sparams.port);
+        fprintf(stderr, "\ncouldn't bind to server socket: hostname=%s port=%d\n\n", sparams.hostname.c_str(),
+                sparams.port);
         return 1;
     }
 
@@ -2549,19 +2610,18 @@ int main(int argc, char **argv)
                                       });
 
     // run the HTTP server in a thread - see comment below
-    std::thread t([&]()
-            {
-                if (!svr.listen_after_bind())
-                {
-                    return 1;
-                }
+    std::thread t([&]() {
+        if (!svr.listen_after_bind())
+        {
+            return 1;
+        }
 
-                return 0;
-            });
+        return 0;
+    });
 
     // GG: if I put the main loop inside a thread, it crashes on the first request when build in Debug!?
     //     "Bus error: 10" - this is on macOS, it does not crash on Linux
-    //std::thread t2([&]()
+    // std::thread t2([&]()
     {
         bool running = true;
         while (running)
